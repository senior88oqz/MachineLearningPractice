{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the following models are built to practice machine translation from English to French:  \n",
    "<li> Simple RNN\n",
    "<li> RNN with embedding\n",
    "<li> Bidirectional RNN\n",
    "<li> Encoder-decoder Model\n",
    "\n",
    "\n",
    "*Reference*:<br> \n",
    "https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd<br>\n",
    "https://github.com/susanli2016/NLP-with-Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import utils\n",
    "import numpy as np\n",
    "import project_tests as tests\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the [data](https://github.com/susanli2016/NLP-with-Python/tree/master/data)**: a *small_vocab_en* file that contains English sentences and their French translations in the *small_vocab_fr*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n"
     ]
    }
   ],
   "source": [
    "english_sents = utils.load_data('data/small_vocab_en.txt')\n",
    "french_sents = utils.load_data('data/small_vocab_fr.txt')\n",
    "print('Dataset loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en line 1: new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "french_vocab_en line 1: new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "----------------------------------------------\n",
      "small_vocab_en line 2: the united states is usually chilly during july , and it is usually freezing in november .\n",
      "french_vocab_en line 2: les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for line_i in range(2):\n",
    "    print('small_vocab_en line %d: %s' % (line_i+1, english_sents[line_i]))\n",
    "    print('french_vocab_en line %d: %s' % (line_i+1, french_sents[line_i]))\n",
    "    print('----------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: the complexity of the problem is determined by the complexity of vocabulary**\n",
    "<br> The below function summarize the vocabulary of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_summary(list_of_sents, lang):\n",
    "    word_counts = Counter()\n",
    "    for sent in list_of_sents:\n",
    "        for word in word_tokenize(sent):\n",
    "            word_counts[word] += 1\n",
    "    vocab = set(word_counts.keys())\n",
    "    print('%d %s words' % (sum(word_counts.values()), lang))\n",
    "    print('%d unique %s words' % (len(vocab), lang))\n",
    "    print('19 most common words:', word_counts.most_common(10))\n",
    "    return word_counts, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1831620 english words\n",
      "200 unique english words\n",
      "19 most common words: [('is', 205882), (',', 140897), ('.', 137049), ('in', 75525), ('it', 75377), ('during', 74933), ('the', 67628), ('but', 63987), ('and', 59850), ('sometimes', 37746)]\n"
     ]
    }
   ],
   "source": [
    "eng_words, eng_vocab = get_word_summary(english_sents, 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000741 french words\n",
      "354 unique french words\n",
      "19 most common words: [('est', 196809), ('.', 137048), (',', 123135), ('en', 105768), ('il', 84079), ('les', 65255), ('mais', 63987), ('et', 59851), ('la', 49861), (\"'\", 38017)]\n"
     ]
    }
   ],
   "source": [
    "french_words, french_vocab = get_word_summary(french_sents, 'french')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "In this part, we will use the one-hot representation with paddings to make all the text sequences the same length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The one-hot encoding part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keras_tokenizer(list_of_strings):\n",
    "    tk = Tokenizer()\n",
    "    tk.fit_on_texts(list_of_strings)\n",
    "    return tk.texts_to_sequences(list_of_strings), tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17, 23, 1, 8, 67, 4, 39, 7, 3, 1, 55, 2, 44], [5, 20, 21, 1, 9, 62, 4, 43, 7, 3, 1, 9, 51, 2, 45], [22, 1, 9, 67, 4, 38, 7, 3, 1, 9, 68, 2, 34], [5, 20, 21, 1, 8, 64, 4, 34, 7, 3, 1, 57, 2, 42], [29, 12, 16, 13, 1, 5, 82, 6, 30, 12, 16, 1, 5, 83], [31, 11, 13, 1, 5, 84, 6, 30, 11, 1, 5, 82], [18, 1, 66, 4, 47, 6, 3, 1, 9, 62, 2, 43], [17, 23, 1, 60, 4, 35, 7, 3, 1, 10, 68, 2, 38], [49, 12, 16, 13, 1, 5, 85, 6, 30, 12, 16, 1, 5, 82], [5, 20, 21, 1, 8, 60, 4, 36, 7, 3, 1, 8, 56, 2, 45]]\n"
     ]
    }
   ],
   "source": [
    "eng_indexed_seq, eng_tokenizer = keras_tokenizer(english_sents)\n",
    "print(eng_indexed_seq[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The padding part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keras_padding(sequence, length = None):\n",
    "    if length is None:\n",
    "        length = max([len(sent) for sent in sequence])\n",
    "    return pad_sequences(sequence, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17, 23,  1, ..., 44,  0,  0],\n",
       "       [ 5, 20, 21, ..., 51,  2, 45],\n",
       "       [22,  1,  9, ..., 34,  0,  0],\n",
       "       ..., \n",
       "       [24,  1, 10, ..., 54,  0,  0],\n",
       "       [ 5, 84,  1, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_padding(eng_indexed_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
