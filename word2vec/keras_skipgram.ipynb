{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram Implementation in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Author Name:Hongxiang Yang<b/>\n",
    "\n",
    "<b>Email: hongxiangy@student.unimelb.edu.au<b/>\n",
    "\n",
    "<b>Python version used:Python 3.5.4 :: Anaconda, Inc.<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import time\n",
    "import keras\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "from collections import Counter\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.python.client import device_lib\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with GPU \n",
    "(Envirnment setup: https://github.com/senior88oqz/dlwin.git)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5485759225754551552\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1557040332\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 13003114433564137188\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 950, pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implements function to clean, preprocess raw text data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_unk(word, vocab):\n",
    "    \"\"\"\n",
    "    :return: replacing uncapctured word as 'UNK'\n",
    "    \"\"\"\n",
    "    return word if word in vocab else 'UNK'\n",
    "\n",
    "\n",
    "# ntlk.download(): stopwords, punkt\n",
    "def preprocessing(text, vocab_size):\n",
    "    \"\"\"\n",
    "    :param text: string \n",
    "    :param vocab_size: int \n",
    "    :return: processed token(list), word_count(Counter), vocab(list)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuations = set(string.punctuation)\n",
    "    exclusions = stop_words.union(punctuations)\n",
    "    token = word_tokenize(text)\n",
    "    processed = [word.lower()\n",
    "                 for word in token\n",
    "                 if word.lower() not in exclusions]\n",
    "    vocab = [word_count[0]\n",
    "             for word_count in Counter(processed).most_common(vocab_size - 1)]\n",
    "    if vocab_size < len(processed):\n",
    "        vocab.insert(0, 'UNK')\n",
    "    processed = [get_unk(word, vocab) for word in processed]\n",
    "    word_count = Counter(processed)\n",
    "    print('Preprocessing done in %s seconds' % ((time.time() - start_time)))\n",
    "    return processed, word_count, vocab\n",
    "\n",
    "\n",
    "def get_lookup_tables(vocab):\n",
    "    \"\"\"\n",
    "    building word <-> index dictionaries for given vocab\n",
    "    :param vocab: \n",
    "    :return: word_index(dict), index_word(dict)\n",
    "    \"\"\"\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "    for index, word in enumerate(vocab):\n",
    "        word_index.setdefault(word, index)\n",
    "        index_word.setdefault(index, word)\n",
    "    return word_index, index_word\n",
    "\n",
    "\n",
    "def get_indexed_text(tokens, word_index):\n",
    "    \"\"\"\n",
    "    One-hot encoding text tokens\n",
    "    :param tokens: text tokens obtained from previous function \n",
    "    :param word_index: \n",
    "    :return: list of text in one-hot encoding\n",
    "    \"\"\"\n",
    "    indexed_text = [word_index[word] for word in tokens]\n",
    "    return indexed_text\n",
    "\n",
    "\n",
    "def build_data_set(text, vocab_size):\n",
    "    \"\"\"\n",
    "    Warpup the above functions and prepare the data_set for training\n",
    "    \"\"\"\n",
    "    processed, word_count, vocab = preprocessing(text, vocab_size)\n",
    "    word_index, index_word = get_lookup_tables(vocab)\n",
    "    indexed_text = get_indexed_text(processed, word_index)\n",
    "    del vocab\n",
    "    return indexed_text, word_count, word_index, index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 3000\n",
    "window_size = 5\n",
    "vec_dim = 300\n",
    "epochs = 10\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done in 23.17365074157715 seconds\n"
     ]
    }
   ],
   "source": [
    "raw_text = ' '.join(brown.words()[:])\n",
    "indexed_text, word_count, word_index, index_word = build_data_set(raw_text, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data setup for keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Subsampling word from text\n",
    "# i.e. word_sample_table[i] is the prob. of sampling ith most common word\n",
    "# more common -> lower prob.\n",
    "word_sample_table = sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "word_pairs, labels = skipgrams(indexed_text, vocab_size, shuffle=True, negative_samples=2,\n",
    "                               window_size=window_size, sampling_table=word_sample_table)\n",
    "\n",
    "word_target, word_context = zip(*word_pairs)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builds up the training models with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "target_input (InputLayer)        (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "context_input (InputLayer)       (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding (Embedding)            (None, 1, 300)        900000      target_input[0][0]               \n",
      "                                                                   context_input[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "taget_reshape (Reshape)          (None, 300, 1)        0           embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "context_reshape (Reshape)        (None, 300, 1)        0           embedding[1][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dot_product (Dot)                (None, 1, 1)          0           taget_reshape[0][0]              \n",
      "                                                                   context_reshape[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dot_product_reshape (Reshape)    (None, 1)             0           dot_product[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "sigmoid (Dense)                  (None, 1)             2           dot_product_reshape[0][0]        \n",
      "====================================================================================================\n",
      "Total params: 900,002\n",
      "Trainable params: 900,002\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# # input layer\n",
    "input_target = Input((1,), name='target_input')\n",
    "input_context = Input((1,), name='context_input')\n",
    "\n",
    "# # embedding layer\n",
    "embedding = Embedding(vocab_size, vec_dim, input_length=1, name='embedding')\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vec_dim, 1), name='taget_reshape')(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vec_dim, 1), name='context_reshape')(context)\n",
    "\n",
    "# # now perform the dot product operation to get a similarity measure\n",
    "dot_product = keras.layers.dot([target, context], axes=1,\n",
    "                               name='dot_product', normalize=True)\n",
    "dot_product = Reshape((1,), name='dot_product_reshape')(dot_product)\n",
    "# # add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid', name='sigmoid')(dot_product)\n",
    "# # create the primary training model\n",
    "model = Model(input=[input_target, input_context], output=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.summary()\n",
    "# # intermediate layer to compute cosine similarity between two words\n",
    "validation_model = Model(input=[input_target, input_context], output=dot_product)\n",
    "# # intermediate layer to convert one-hot code (sparse) into word embeddings (dense) \n",
    "# # which can be used in many other nlp tasks\n",
    "vec_model = Model(input=input_target, output=target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "36s - loss: 0.6022 - acc: 0.6938\n",
      "Epoch 2/10\n",
      "35s - loss: 0.5140 - acc: 0.7469\n",
      "Epoch 3/10\n",
      "35s - loss: 0.4652 - acc: 0.7830\n",
      "Epoch 4/10\n",
      "35s - loss: 0.4276 - acc: 0.8103\n",
      "Epoch 5/10\n",
      "35s - loss: 0.3961 - acc: 0.8307\n",
      "Epoch 6/10\n",
      "35s - loss: 0.3690 - acc: 0.8464\n",
      "Epoch 7/10\n",
      "35s - loss: 0.3453 - acc: 0.8594\n",
      "Epoch 8/10\n",
      "35s - loss: 0.3242 - acc: 0.8701\n",
      "Epoch 9/10\n",
      "35s - loss: 0.3050 - acc: 0.8792\n",
      "Epoch 10/10\n",
      "35s - loss: 0.2883 - acc: 0.8868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25c97d389b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([word_target, word_context], labels,\n",
    "          validation_split=0,\n",
    "          batch_size=batch_size, epochs=epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of the model\n",
    "- word2vec: produce word embeding for a string word\n",
    "- skipgram: given a target word return the top k likely word in a predefine window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2vec(word, vec_model, word_index=word_index):\n",
    "    index = np.array([word_index[word]])\n",
    "    return vec_model.predict_on_batch(index).flatten()\n",
    "\n",
    "\n",
    "\n",
    "def get_k_most_common_context(target, validation_model, top_k,\n",
    "                              lookups=(word_index, index_word), vocab_size=vocab_size):\n",
    "    word_index, index_word = lookups\n",
    "    target_indx = np.array([word_index[target]])\n",
    "    similarities = np.zeros((vocab_size,))\n",
    "    for i in range(vocab_size):\n",
    "        context_indx = np.array([i])\n",
    "        sim = validation_model.predict_on_batch([target_indx, context_indx])\n",
    "        similarities[i] = sim\n",
    "    nearest = (-similarities).argsort()[1:top_k + 1]\n",
    "    out = [index_word[word] for word in nearest]\n",
    "    print(\"%d Nearest to %s :\" % (top_k, target), out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension:  300\n",
      "[ 0.0023099  -0.0469316   0.26281762  0.04457152  0.15659074 -0.19045512\n",
      " -0.14653884 -0.16654626 -0.16516408  0.05719168  0.0065063  -0.18198052\n",
      " -0.03592599 -0.04285887  0.04855128 -0.07246569 -0.14164248  0.00507005\n",
      " -0.06912898 -0.25567499 -0.08014426  0.13297312 -0.06913213 -0.13081819\n",
      " -0.15918715  0.15340981 -0.23020993  0.17676426  0.02816044 -0.29095277\n",
      "  0.14475267  0.01372892  0.06072622  0.13990726 -0.36064938 -0.19122009\n",
      "  0.05285533  0.26722857  0.17350371 -0.28393173  0.08968197 -0.3094438\n",
      "  0.33192924  0.02098076  0.0808959  -0.20219873 -0.00732423 -0.2924397\n",
      "  0.10726909  0.06473857 -0.12077001  0.40374848  0.12536813 -0.08836307\n",
      "  0.48754469  0.03483732 -0.0346945   0.03701563  0.04345041  0.0367719\n",
      "  0.10236261 -0.25089979 -0.04918829  0.04756913 -0.06396046 -0.26823333\n",
      "  0.11190939 -0.05368825 -0.05699518  0.14960416  0.1264948   0.15372215\n",
      "  0.05643407  0.30918989  0.08186588  0.37794787  0.05138092  0.21816196\n",
      " -0.1275795   0.07869629 -0.05768437  0.07829168 -0.06935058 -0.30980086\n",
      " -0.2743144  -0.11739255 -0.44916871 -0.09743027 -0.08041051  0.05403317\n",
      " -0.10401992 -0.01863268 -0.11609486  0.05083385  0.07341085 -0.06096105\n",
      "  0.1312152  -0.16895977 -0.36584386 -0.06652548 -0.22452848 -0.09149858\n",
      " -0.01947818  0.0268438   0.15614954 -0.12370907  0.24782373 -0.08926848\n",
      "  0.11864647  0.19674334 -0.04927952 -0.0149095   0.11994486  0.09872041\n",
      " -0.14522275 -0.18187486 -0.40954566 -0.03260331  0.0825113  -0.04457578\n",
      "  0.32440317  0.00297513 -0.07044316  0.12698071 -0.06179741  0.31358942\n",
      " -0.00451902  0.09521025 -0.10997046  0.06335675  0.17110707 -0.17061873\n",
      "  0.23651889  0.32005301  0.02802914 -0.04379199  0.20560582 -0.0063902\n",
      " -0.03690393  0.01237225 -0.05649905  0.14275052  0.07709192  0.1224739\n",
      "  0.25058421  0.02200764 -0.01920495 -0.23097983  0.08511389 -0.08478727\n",
      "  0.08238843 -0.07766868  0.25904915  0.1571447  -0.39860871  0.04785737\n",
      "  0.22685963  0.13995892 -0.27413446  0.25868255 -0.1481903   0.01620858\n",
      "  0.0296745   0.18213226 -0.11097443 -0.13386855 -0.22977288 -0.16187927\n",
      "  0.20484577 -0.00671512 -0.07590333 -0.13713898  0.21646574  0.07220626\n",
      "  0.11190057  0.21064718  0.02043815  0.04262435  0.07403705 -0.02996617\n",
      " -0.05588552 -0.07518377  0.20994018  0.11533023 -0.02506421  0.27425304\n",
      "  0.18514994  0.32059059 -0.13665795  0.34134024 -0.17938657  0.17652358\n",
      "  0.06775885 -0.08232947 -0.19422066 -0.19348174  0.28476301 -0.00169362\n",
      " -0.11805297 -0.10047228 -0.29062071 -0.03843006 -0.17671573  0.1999073\n",
      "  0.46489286 -0.06484655 -0.28902334  0.07063837 -0.21391256 -0.07726153\n",
      " -0.16947983 -0.07320505  0.33932346  0.02281918  0.12662423 -0.07142496\n",
      "  0.15516609 -0.20444499 -0.05281293  0.2056984   0.15828952  0.3516472\n",
      " -0.31001455  0.06105386  0.06205953  0.11982042  0.13597749  0.1840795\n",
      " -0.10860643  0.1108547   0.12822539 -0.06170715 -0.08861427 -0.2929723\n",
      " -0.24540621 -0.19136404  0.21154577 -0.10922202  0.02919577 -0.16286513\n",
      " -0.07098497  0.13952439 -0.15810125 -0.31388685 -0.06669409 -0.04470532\n",
      " -0.0352936   0.06652027 -0.19519533  0.06436297 -0.10676212 -0.03847823\n",
      " -0.50341451  0.07520602  0.16879356  0.15152708 -0.17650706 -0.34010068\n",
      " -0.407199   -0.11447255 -0.17909244  0.40693241 -0.36347312  0.11444668\n",
      " -0.20542915  0.09875532 -0.30856794  0.40776747  0.01184688 -0.21816334\n",
      " -0.16383842  0.19193687  0.06989008  0.21981138 -0.05396944 -0.01560418\n",
      "  0.00061318 -0.19290236  0.08283724 -0.08815049  0.13698731 -0.17407717\n",
      " -0.36878616  0.19488034 -0.15900664  0.48559925  0.01215182 -0.13396269\n",
      "  0.0023455   0.09453174 -0.01193264  0.01556791 -0.14296181  0.03234515\n",
      "  0.05531725 -0.05125317  0.21625192  0.05775717  0.1826048   0.13464642]\n"
     ]
    }
   ],
   "source": [
    "vec = word2vec('milk', vec_model)\n",
    "print(\"dimension: \", len(vec))\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Nearest to white milk : [\"'s\", 'breakfast', '``', 'providing', 'put', 'production', 'feed', 'foods']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"'s\", 'breakfast', '``', 'providing', 'put', 'production', 'feed', 'foods']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_most_common_context('milk', validation_model, top_k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
