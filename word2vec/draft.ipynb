{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done in 3.7397754192352295 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "import zipfile\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Merge, Dot\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import urllib\n",
    "import collections\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "\n",
    "dataset_folder_path = 'data'\n",
    "dataset_filename = 'text8.zip'\n",
    "dataset_name = 'Text8 Dataset'\n",
    "\n",
    "def get_unk(word, vocab):\n",
    "    return word if word in vocab else 'UNK'\n",
    "\n",
    "# ntlk.download(): stopwords, punkt\n",
    "def preprocessing(text, vocab_size):\n",
    "    start_time = time.time()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuations = set(string.punctuation)\n",
    "    exclusions = stop_words.union(punctuations)\n",
    "    token = word_tokenize(text)\n",
    "    processed = [word.lower()\n",
    "                 for word in token\n",
    "                 if word.lower() not in exclusions]\n",
    "    vocab = [word_count[0]\n",
    "                for word_count in Counter(processed).most_common(vocab_size-1)]\n",
    "    if vocab_size < len(processed):\n",
    "        vocab.insert(0, 'UNK')\n",
    "    processed = [get_unk(word, vocab) for word in processed]\n",
    "    word_count = Counter(processed)\n",
    "    print('Preprocessing done in %s seconds' % ((time.time() - start_time)))\n",
    "    return processed, word_count, vocab\n",
    "\n",
    "def get_lookup_tables(vocab):\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "    for index, word in enumerate(vocab):\n",
    "        word_index.setdefault(word, index)\n",
    "        index_word.setdefault(index, word)\n",
    "    return word_index, index_word\n",
    "\n",
    "def get_indexed_text(text, word_index):\n",
    "    indexed_text =  [word_index[word] for word in text]\n",
    "    return indexed_text\n",
    "\n",
    "def build_data_set(text, vocab_size):\n",
    "    processed, word_count, vocab = preprocessing(text, vocab_size)\n",
    "    word_index, index_word = get_lookup_tables(vocab)\n",
    "    indexed_text = get_indexed_text(processed, word_index)\n",
    "    del vocab\n",
    "    return indexed_text, word_count, word_index, index_word\n",
    "\n",
    "def get_subSample(text):\n",
    "    threshold = 1e-5 #how?\n",
    "    all_count = len(text)\n",
    "    word_freq = {word: count/all_count\n",
    "                 for word, count in Counter(text).items()}\n",
    "    p_drop = {word: 1-np.sqrt(threshold/word_freq[word])\n",
    "              for word in indexed_text}\n",
    "    subSample = [word for word in text if random.random() < (1 - p_drop[word])]\n",
    "    return subSample\n",
    "\n",
    "def get_surroundings(text, index, window_size=5):\n",
    "    roll = np.random.randint(1, window_size+1)\n",
    "    text_length = len(text)\n",
    "    start = index - roll if (index - roll) > 0 else 0\n",
    "    end = index + roll if (index + roll) < text_length else text_length\n",
    "    surroundings = text[start:index] + text[index+1:end+1]\n",
    "    return surroundings\n",
    "\n",
    "def get_batches(text, batch_size, window_size=5):\n",
    "    n_batch = len(text)//batch_size\n",
    "    text = text[: n_batch*batch_size]\n",
    "    for index in range(0, len(text), batch_size):\n",
    "        X, Y = [], []\n",
    "        batch = text[index: index+batch_size]\n",
    "        for i in range (len(batch)):\n",
    "            x = batch[i]\n",
    "            y = get_surroundings(batch, i, window_size)\n",
    "            X.append(x)\n",
    "            Y.extend(y)\n",
    "        yield X, Y\n",
    "\n",
    "# if not isfile(dataset_filename):\n",
    "#     with DLProgress(unit='B', unit_scale=True, miniters=1, desc=dataset_name) as pbar:\n",
    "#         urlretrieve(\n",
    "#             'http://mattmahoney.net/dc/text8.zip',\n",
    "#             dataset_filename,\n",
    "#             pbar.hook)\n",
    "#\n",
    "# if not isdir(dataset_folder_path):\n",
    "#     with zipfile.ZipFile(dataset_filename) as zip_ref:\n",
    "#         zip_ref.extractall(dataset_folder_path)\n",
    "#\n",
    "# with open('./data/text8') as f:\n",
    "#     raw_text = f.read()\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "raw_text = gutenberg.raw('melville-moby_dick.txt')\n",
    "\n",
    "vocab_size = 1000\n",
    "indexed_text, word_count, word_index, index_word = build_data_set(raw_text, vocab_size)\n",
    "window_size = 5\n",
    "vec_dim = 100\n",
    "epochs = 5\n",
    "valid_size = 16\n",
    "valid_window = 100\n",
    "valid_sample = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "# Subsampling word from text\n",
    "# i.e. word_sample_table[i] is the prob. of sampling ith most common word\n",
    "# more common -> lower prob.\n",
    "word_sample_table = sequence.make_sampling_table(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampes:  77918\n",
      "positive sample:  38959\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'axes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-cc73411719c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mmerge_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmerged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_model\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# dot_axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mmerge_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mcontext_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got multiple values for argument 'axes'"
     ]
    }
   ],
   "source": [
    "word_pairs, labels = skipgrams(indexed_text, vocab_size, shuffle=True, negative_samples=1,\n",
    "                            window_size=window_size, sampling_table=word_sample_table)\n",
    "print (\"sampes: \", len(labels))\n",
    "print (\"positive sample: \", np.sum(labels))\n",
    "word_target, word_context = zip(*word_pairs)\n",
    "\n",
    "target_model = keras.models.Sequential()\n",
    "target_model.add(Embedding(vocab_size, vec_dim, input_length=1))\n",
    "\n",
    "context_model = keras.models.Sequential()\n",
    "context_model.add(Embedding(vocab_size, vec_dim, input_length=1))\n",
    "\n",
    "merge_model = keras.models.Sequential()\n",
    "merged = Dot([target_model, context_model], axes=1) # dot_axes\n",
    "merge_model.add(merged)\n",
    "context_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras.layers.merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 100)\n",
      "(None, 1, 100)\n",
      "Shape.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "C:\\Users\\Khris\\Anaconda3\\lib\\site-packages\\keras\\legacy\\layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "# create some input variables\n",
    "word_target, word_context = zip(*word_pairs)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, vec_dim, input_length=1, name='embedding')\n",
    "target = embedding(input_target)\n",
    "print(embedding.output_shape)\n",
    "\n",
    "# target = Reshape((vec_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "print(embedding.output_shape)\n",
    "# context = Reshape((vec_dim, 1))(context)\n",
    "\n",
    "dot_product = merge([target, context], mode='dot' , dot_axes=-1)\n",
    "print(dot_product.shape)\n",
    "\n",
    "\n",
    "# # setup a cosine similarity operation which will be output in a secondary model\n",
    "# similarity = merge([target, context], mode='cos', dot_axes=0)\n",
    "\n",
    "# # now perform the dot product operation to get a similarity measure\n",
    "# dot_product = merge([target, context], mode='dot' , dot_axes=1)\n",
    "# dot_product = Reshape((1,))(dot_product)\n",
    "# # add the sigmoid output layer\n",
    "# output = Dense(1, activation='sigmoid')(dot_product)\n",
    "# # create the primary training model\n",
    "# model = Model(input=[input_target, input_context], output=output)\n",
    "# model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# validation_model = Model(input=[input_target, input_context], output=similarity)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers.merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class SimilarityCallback:\n",
    "#     def run_sim(self):\n",
    "#         for i in range(valid_size):\n",
    "#             valid_word = index_word[valid_sample[i]]\n",
    "#             top_k = 8  # number of nearest neighbors\n",
    "#             sim = self._get_sim(valid_sample[i])\n",
    "#             nearest = (-sim).argsort()[1:top_k + 1]\n",
    "#             log_str = 'Nearest to %s:' % valid_word\n",
    "#             for k in range(top_k):\n",
    "#                 close_word = index_word[nearest[k]]\n",
    "#                 log_str = '%s %s,' % (log_str, close_word)\n",
    "#             print(log_str)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _get_sim(valid_word_idx):\n",
    "#         sim = np.zeros((vocab_size,))\n",
    "#         in_arr1 = np.zeros((1,))\n",
    "#         in_arr2 = np.zeros((1,))\n",
    "#         in_arr1[0,] = valid_word_idx\n",
    "#         for i in range(vocab_size):\n",
    "#             in_arr2[0,] = i\n",
    "#             out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "#             sim[i] = out\n",
    "#         return sim\n",
    "# sim_cb = SimilarityCallback()\n",
    "\n",
    "# arr_1 = np.zeros((1,))\n",
    "# arr_2 = np.zeros((1,))\n",
    "# arr_3 = np.zeros((1,))\n",
    "# for cnt in range(epochs):\n",
    "#     idx = np.random.randint(0, len(labels)-1)\n",
    "#     arr_1[0,] = word_target[idx]\n",
    "#     arr_2[0,] = word_context[idx]\n",
    "#     arr_3[0,] = labels[idx]\n",
    "#     loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "#     if cnt % 100 == 0:\n",
    "#         print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "#     if cnt % 10000 == 0:\n",
    "#         sim_cb.run_sim()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
